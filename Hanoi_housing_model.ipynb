{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vusanov/madmodel/blob/main/Hanoi_housing_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load The Initial Dataset**"
      ],
      "metadata": {
        "id": "UL3Fq2Prb0Vh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joAeijSg19FG"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Necessary Libraries**"
      ],
      "metadata": {
        "id": "293v4IUbcFxY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTrTbUXx2PLQ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "from scipy.linalg import null_space\n",
        "import matplotlib.pyplot as plt\n",
        "from sympy import Rational\n",
        "import sympy as sp\n",
        "import random\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pre-Processing Dataset Stage**"
      ],
      "metadata": {
        "id": "O9wSA54EcW0T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 379,
      "metadata": {
        "id": "-r_Zfv7V4IbO"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"Hanoi_housing_dataset.csv\")\n",
        "df[\"Ngày\"] = pd.to_datetime(df[\"Ngày\"])\n",
        "df[\"Ngày\"].dt.year.value_counts()\n",
        "df[\"Ngày\"] = df[\"Ngày\"].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 380,
      "metadata": {
        "id": "odPdxKmV5CPr"
      },
      "outputs": [],
      "source": [
        "df = df.rename(columns = {\"Ngày\":\"date\", \"Địa chỉ\":\"address\", \"Quận\":\"district\",\n",
        "                                  \"Huyện\":\"ward\", \"Loại hình nhà ở\":\"type_of_housing\",\n",
        "                                 \"Giấy tờ pháp lý\":\"legal_paper\", \"Số tầng\":\"num_floors\",\n",
        "                                 \"Số phòng ngủ\":\"num_bed_rooms\", \"Diện tích\":\"squared_meter_area\",\n",
        "                                 \"Dài\":\"length_meter\", \"Rộng\":\"width_meter\", \"Giá/m2\":\"price_in_million_per_square_meter\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 381,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9olrcMm47N8",
        "outputId": "bb1e7d78-036c-4ebc-d01a-384fec5e0e5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total records of the dataset are:  11473 records.\n"
          ]
        }
      ],
      "source": [
        "df = df.drop(\"Unnamed: 0\", axis = 1)\n",
        "df = df.dropna()\n",
        "df = df.reset_index()\n",
        "\n",
        "print(\"The total records of the dataset are: \", str(len(df)), \"records.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 382,
      "metadata": {
        "id": "ffL-s0CT4DaI"
      },
      "outputs": [],
      "source": [
        "df = df[df['num_floors'] != 'Nhiều hơn 10']\n",
        "df = df[df['num_bed_rooms'] != 'nhiều hơn 10 phòng']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 383,
      "metadata": {
        "id": "LcWx6THJ6RaJ"
      },
      "outputs": [],
      "source": [
        "df['district'] = df['district'].str.replace('Quận ','').str.strip()\n",
        "df['ward'] = df['ward'].str.replace('Phường ','').str.strip()\n",
        "df['num_floors'] = df['num_floors'].str.strip().astype(float)\n",
        "df['num_bed_rooms'] = df['num_bed_rooms'].str.replace(' phòng','').str.strip().astype(float)\n",
        "df['squared_meter_area'] = df['squared_meter_area'].str.replace(' m²','').str.strip().astype(float)\n",
        "df['length_meter'] = df['length_meter'].str.replace(' m','').str.strip().astype(float)\n",
        "df['width_meter'] = df['width_meter'].str.replace(' m','').str.strip().astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 384,
      "metadata": {
        "id": "KHKLaeBC4Dgz"
      },
      "outputs": [],
      "source": [
        "df.loc[df['price_in_million_per_square_meter'].str.contains(' tỷ/m²'), 'price_in_million_per_square_meter'] = df.loc[df['price_in_million_per_square_meter'].str.contains(' tỷ/m²'), 'price_in_million_per_square_meter'].str.replace(' tỷ/m²','').str.replace('.','').str.replace(',','.').astype(float) * 1000\n",
        "df.loc[df['price_in_million_per_square_meter'].str.contains(' triệu/m²', na=False), 'price_in_million_per_square_meter'] = df.loc[df['price_in_million_per_square_meter'].str.contains(' triệu/m²', na=False), 'price_in_million_per_square_meter'].str.replace(' triệu/m²','').str.replace(',','.').astype(float)\n",
        "df.loc[df['price_in_million_per_square_meter'].str.contains(' đ/m²', na=False), 'price_in_million_per_square_meter'] = df.loc[df['price_in_million_per_square_meter'].str.contains(' đ/m²', na=False), 'price_in_million_per_square_meter'].str.replace(' đ/m²','').str.replace('.','').astype(float) * 0.000001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 385,
      "metadata": {
        "id": "uj6Fm8VJ4D1h"
      },
      "outputs": [],
      "source": [
        "# Use One-hot encoding to classify features such as \"Loại nhà ở\", \"Giấy tờ pháp lý\", \"Quận\", \"Huyện\"\n",
        "\n",
        "dummy_type_of_housing = pd.get_dummies(df.type_of_housing, prefix=\"housing_type\")\n",
        "dummy_legal_paper = pd.get_dummies(df.legal_paper, prefix=\"legal_paper\")\n",
        "dummy_district = pd.get_dummies(df.district, prefix=\"district\")\n",
        "dummy_ward = pd.get_dummies(df.ward, prefix=\"ward\")\n",
        "\n",
        "df_cleaned = pd.concat([df, dummy_type_of_housing, dummy_legal_paper, dummy_district, dummy_ward], axis=1)\n",
        "df_cleaned = df_cleaned.drop(['index', 'date', 'address', 'district', 'ward', 'type_of_housing', 'legal_paper'], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 386,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fUUVieI4EDA",
        "outputId": "8c1c0780-f32d-4813-cc67-a5170b55b3c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The final length of the dataset is 7328 rows.\n"
          ]
        }
      ],
      "source": [
        "# Remove outlier values\n",
        "\n",
        "def remove_outlier_IQR(df, series):\n",
        "    Q1=df[series].quantile(0.25)\n",
        "    Q3=df[series].quantile(0.75)\n",
        "    IQR=Q3-Q1\n",
        "    df_final=df[~((df[series]<(Q1-1.5*IQR)) | (df[series]>(Q3+1.5*IQR)))]\n",
        "    return df_final\n",
        "\n",
        "removed_outliers = df_cleaned\n",
        "columns_to_remove_outliers = ['num_floors', 'num_bed_rooms', 'squared_meter_area', 'length_meter',\n",
        "                              'width_meter', 'price_in_million_per_square_meter']\n",
        "for column in columns_to_remove_outliers:\n",
        "    removed_outliers = remove_outlier_IQR(removed_outliers, column)\n",
        "\n",
        "print(\"The final length of the dataset is\", str(len(removed_outliers)), \"rows.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xfmai94y4ELl"
      },
      "outputs": [],
      "source": [
        "data = removed_outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Split The Hanlded Data Into Training, Testing Set And Normalize them using StandardScaler**"
      ],
      "metadata": {
        "id": "Ql0St7PhgiGK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNVi_n0i_g35"
      },
      "outputs": [],
      "source": [
        "n = int(0.8*len(data))\n",
        "x = data.loc[:, (data.columns != 'price_in_million_per_square_meter') & (data.columns != 'date')]\n",
        "y = data[['price_in_million_per_square_meter']]\n",
        "to_be_scaled = ['num_floors', 'num_bed_rooms', 'squared_meter_area', 'length_meter', 'width_meter']\n",
        "\n",
        "PredictorScaler=StandardScaler()\n",
        "TargetVarScaler=StandardScaler()\n",
        "\n",
        "x_scaled = x\n",
        "y_scaled = y\n",
        "\n",
        "# Storing the fit object for reference and reverse the scaling later\n",
        "PredictorScalerFit=PredictorScaler.fit(x_scaled[to_be_scaled])\n",
        "TargetVarScalerFit=TargetVarScaler.fit(y_scaled)\n",
        "\n",
        "x_scaled[to_be_scaled]=PredictorScalerFit.transform(x_scaled[to_be_scaled])\n",
        "y_scaled=TargetVarScalerFit.transform(y)\n",
        "\n",
        "x_arr = np.array(x_scaled.values).astype(\"float32\")\n",
        "y_arr = np.array(y_scaled).astype(\"float32\")\n",
        "\n",
        "xtr, ytr = torch.tensor(x_arr[:n], dtype=torch.float32), torch.tensor(y_arr[:n], dtype=torch.float32)\n",
        "xte, yte = torch.tensor(x_arr[n:], dtype=torch.float32), torch.tensor(y_arr[n:], dtype=torch.float32)\n",
        "print((xtr.shape, ytr.shape), (xte.shape, yte.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build The Model And Necessary Technique Layers**"
      ],
      "metadata": {
        "id": "CdoONjzKcmx2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esGAr0ZrDHpW"
      },
      "outputs": [],
      "source": [
        "# CosineAnnealingLR method for controlling learning rate during each training step\n",
        "class CosineAnnealingLR:\n",
        "  def __init__(self, initial_lr, max_steps, min_lr=1e-6):\n",
        "    self.initial_lr = initial_lr\n",
        "    self.max_steps = max_steps\n",
        "    self.min_lr = min_lr\n",
        "\n",
        "  def get_lr(self, current_step):\n",
        "    curr = (1 + torch.cos(torch.pi * current_step / self.max_steps)) / 2\n",
        "    return self.min_lr + (self.initial_lr - self.min_lr) * curr\n",
        "\n",
        "# Linear class\n",
        "class Linear:\n",
        "  def __init__(self, fan_in, fan_out, bias=True):\n",
        "    self.weight = torch.randn((fan_in, fan_out), generator=g) * (2**0.5) / ((fan_in)**0.5)\n",
        "    self.bias = torch.zeros(fan_out) if bias else None\n",
        "\n",
        "  def __call__(self, x):\n",
        "    self.out = x @ self.weight\n",
        "    if self.bias is not None:\n",
        "      self.out += self.bias\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
        "\n",
        "# Batch normalization technique\n",
        "class BatchNorm1d:\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.momentum = momentum\n",
        "    self.training = True\n",
        "    self.gamma = torch.ones(dim) #- 0.70\n",
        "    self.beta = torch.zeros(dim)\n",
        "    self.running_mean = torch.zeros(dim)\n",
        "    self.running_var = torch.ones(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    if self.training:\n",
        "      xmean = x.mean(0, keepdim=True) # batch mean\n",
        "      xvar = x.var(0, keepdim=True) # batch variance\n",
        "    else:\n",
        "      xmean = self.running_mean\n",
        "      xvar = self.running_var\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    # update the buffers\n",
        "    if self.training:\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
        "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "# Tanh activation function\n",
        "class tanh:\n",
        "  def __call__(self, x):\n",
        "    self.out = torch.tanh(x)\n",
        "    return self.out\n",
        "  def parameters(self):\n",
        "    return []\n",
        "\n",
        "# ReLU activation function\n",
        "class relu:\n",
        "  def __call__(self, x):\n",
        "    self.out = torch.relu(x)\n",
        "    return self.out\n",
        "  def parameters(self):\n",
        "    return []\n",
        "\n",
        "# GELU activation function\n",
        "class gelu:\n",
        "  def __call__(self, x):\n",
        "    self.out =  0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) *(x + 0.044715 * torch.pow(x, 3))))\n",
        "    return self.out\n",
        "  def parameters(self):\n",
        "    return []\n",
        "\n",
        "# Residual Connection layer to avoid vanishing-gradient phenomenon during backpropagation process\n",
        "class ResidualLayer:\n",
        "  def __init__(self, block):\n",
        "    self.block = block\n",
        "    self.out = None\n",
        "\n",
        "  def __call__(self, x):\n",
        "    identity = x\n",
        "    out_temp = x\n",
        "    for layer in self.block:\n",
        "      out_temp = layer(out_temp)\n",
        "    self.out = out_temp + identity\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [p for layer in self.block for p in layer.parameters()]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Initialize The Model And Related Configurations**"
      ],
      "metadata": {
        "id": "QcOM625Beziq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = xtr.shape[1]\n",
        "output = 1\n",
        "n_hidden = 32 # The amount of neurons in hidden layers\n",
        "mae_loss = nn.L1Loss() # Correspond to the mean absolute error (MAE)\n",
        "g = torch.Generator().manual_seed(2147483647) # Reproducibility\n",
        "\n",
        "# The model\n",
        "layers = [\n",
        "   Linear(input, n_hidden, bias=False), relu(),\n",
        "   ResidualLayer([Linear(n_hidden, n_hidden, bias=False), relu()]),\n",
        "   Linear(n_hidden, n_hidden, bias=False), relu(),\n",
        "   ResidualLayer([Linear(n_hidden, n_hidden, bias=False), relu()]),\n",
        "   Linear(n_hidden, output, bias=False),\n",
        "]\n",
        "\n",
        "'''layers = [\n",
        "   Linear(input, n_hidden, bias=False), BatchNorm1d(n_hidden), relu(),\n",
        "   ResidualLayer([Linear(n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), relu()]),\n",
        "   Linear(n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), relu(),\n",
        "   ResidualLayer([Linear(n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), relu()]),\n",
        "   Linear(n_hidden, output, bias=False),\n",
        "]'''\n",
        "\n",
        "# Store all model parameters and set 'requires_grad = True' for turning them into trainable weights during training\n",
        "parameters = [p for layer in layers for p in layer.parameters()]\n",
        "print(np.sum([p.nelement() for p in parameters]))\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "id": "58v2A6l0eysB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create this function for taking all sub-layers that inside the Residual Connection layer\n",
        "def get_all_layers(layers_list):\n",
        "  expanded_list = []\n",
        "  for layer in layers_list:\n",
        "    expanded_list.append(layer)\n",
        "    if hasattr(layer, 'block'):\n",
        "      expanded_list.extend(get_all_layers(layer.block))\n",
        "  return expanded_list"
      ],
      "metadata": {
        "id": "qhAhV_Nl9fh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_all_layers(layers))"
      ],
      "metadata": {
        "id": "SSG_6FoP_rjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyI4z6MJDnJi"
      },
      "outputs": [],
      "source": [
        "# Try making predictions with the model before training\n",
        "out = xtr\n",
        "for layer in layers:\n",
        "  out = layer(out)\n",
        "\n",
        "y_pred = TargetVarScalerFit.inverse_transform(out.detach())\n",
        "ytr_orig = TargetVarScalerFit.inverse_transform(ytr)\n",
        "\n",
        "# Display predicted results using Pandas\n",
        "TestingData = pd.DataFrame(data=xtr, columns=x.columns)\n",
        "TestingData['Truth label'] = ytr_orig\n",
        "TestingData['Predicted label'] = y_pred\n",
        "TestingData[['Truth label', 'Predicted label']][:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d05WAniGDtpI"
      },
      "outputs": [],
      "source": [
        "max_steps = 100000\n",
        "batch_size = 128\n",
        "initial_lr = 0.1 # I'll set initial lr = 0.1 and gradually reduce lr according to the Cosine function curve.\n",
        "all_layers_flat = get_all_layers(layers)\n",
        "scheduler = CosineAnnealingLR(initial_lr=initial_lr, max_steps=torch.tensor(max_steps, dtype=torch.float32)) # Initialize CosineAnnealingLR\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # minibatch construct if we use Batch Normalization technique, other wise, just using the original xtr, ytr\n",
        "  #idx = torch.randint(0, xtr.shape[0], (batch_size,), generator=g)\n",
        "  #Xb, Yb = xtr[idx], ytr[idx] # batch X,Y\n",
        "\n",
        "  # forward pass\n",
        "  out = xtr\n",
        "  for layer in layers:\n",
        "    out = layer(out)\n",
        "  y_pred = out\n",
        "  loss = mae_loss(y_pred, ytr.reshape(-1, 1)) # loss function\n",
        "\n",
        "  # backward pass\n",
        "  for layer in all_layers_flat:\n",
        "    if hasattr(layer, 'out') and layer.out is not None:\n",
        "      layer.out.retain_grad() # Get the gradient in all layers (include all sub-layers of the Residual layers) for drawing the gradient graph\n",
        "\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  #lr = 0.1 if i < 5000 else 0.01 # step learning rate decay\n",
        "  lr = scheduler.get_lr(i)\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  if i % 10000 == 0 or i == 99999:\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f} | lr: {lr}')\n",
        "  #break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKjrhKEPJkuY"
      },
      "outputs": [],
      "source": [
        "# All experiments with different techniques\n",
        "\n",
        "#100000 loops (with 1 ReLU() layer, n_hidden=15, lr=0.1, He init) -> error: 0.3652\n",
        "#100000 loops (with 1 ReLU() layer, n_hidden=32, lr=0.1, He init) -> error: 0.2901\n",
        "#100000 loops (with 2 ReLU() layers, n_hidden=15, lr=0.1, He init) -> error: 0.3112\n",
        "#100000 loops (with 2 ReLU() layers, n_hidden=32, lr=0.1, He init) -> error: 0.1701\n",
        "\n",
        "#100000 loops (with 1 ReLU() layer, 1 Residual layer, n_hidden=15, lr=0.1, He init) -> error: 0.3193\n",
        "#100000 loops (with 1 ReLU() layer, 1 Residual layer, n_hidden=32, lr=0.1, He init) -> error: 0.1695\n",
        "#100000 loops (with 2 ReLU() layer, 2 Residual layer, n_hidden=15, lr=0.1, He init) -> error: 0.2658\n",
        "#100000 loops (with 2 ReLU() layer, 2 Residual layer, n_hidden=32, lr=0.1, He init) -> error: 0.0808\n",
        "\n",
        "# Note: For Batch Normalization, i think we should use fixed learning rate due to its noise through batches, but i still use consineLR due to my laziness\n",
        "#100000 loops (with 1 ReLU() layer, 1 Batchnorm layer (64 batches), n_hidden=15, lr=0.1, He init) -> error: 0.47256 (mean)\n",
        "#100000 loops (with 1 ReLU() layer, 1 Batchnorm layer (64 batches), n_hidden=32, lr=0.1, He init) -> error: 0.44763 (mean)\n",
        "#100000 loops (with 1 ReLU() layer, 1 Batchnorm layer (128 batches), n_hidden=15, lr=0.1, He init) -> error: 0.44629 (mean)\n",
        "#100000 loops (with 1 ReLU() layer, 1 Batchnorm layer (128 batches), n_hidden=32, lr=0.1, He init) -> error: 0.39471 (mean)\n",
        "#100000 loops (with 2 ReLU() layer, 2 Batchnorm layer (64 batches), n_hidden=15, lr=0.1, He init) -> error: 0.49454 (mean)\n",
        "#100000 loops (with 2 ReLU() layer, 2 Batchnorm layer (64 batches), n_hidden=32, lr=0.1, He init) -> error: 0.40521 (mean)\n",
        "#100000 loops (with 2 ReLU() layer, 2 Batchnorm layer (128 batches), n_hidden=15, lr=0.1, He init) -> error: 0.46335 (mean)\n",
        "#100000 loops (with 2 ReLU() layer, 2 Batchnorm layer (128 batches), n_hidden=32, lr=0.1, He init) -> error: 0.37837 (mean)\n",
        "\n",
        "#100000 loops (with 1 ReLU() layer, 1 Batchnorm layer (128 batches), 1 Residual layer, n_hidden=15, lr=0.1, He init) -> error: 0.52095 (mean)\n",
        "#100000 loops (with 1 ReLU() layer, 1 Batchnorm layer (128 batches), 1 Residual layer, n_hidden=32, lr=0.1, He init) -> error: 0.40776 (mean)\n",
        "#100000 loops (with 2 ReLU() layer, 2 Batchnorm layer (128 batches), 2 Residual layer, n_hidden=15, lr=0.1, He init) -> error: 0.47726 (mean)\n",
        "#100000 loops (with 2 ReLU() layer, 2 Batchnorm layer (128 batches), 2 Residual layer, n_hidden=32, lr=0.1, He init) -> error: 0.34930 (mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxNPaKgcZLi1"
      },
      "outputs": [],
      "source": [
        "# Gradient distribution through all layers after 100000 loops\n",
        "plt.figure(figsize=(20, 4))\n",
        "legends = []\n",
        "for i, layer in enumerate(layers[:]):\n",
        "  t = layer.out.grad\n",
        "  print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))\n",
        "  hy, hx = torch.histogram(t, density=True)\n",
        "  plt.plot(hx[:-1].detach(), hy.detach())\n",
        "  legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
        "plt.legend(legends);\n",
        "plt.title('gradient distribution')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MD4sXveobNa"
      },
      "outputs": [],
      "source": [
        "# Print the gradients of all Linear layers\n",
        "for i, layer in enumerate(layers[:-1]):\n",
        "  if isinstance(layer, Linear):\n",
        "    t = layer.out.grad\n",
        "    print(t.min(), t.max())\n",
        "    print(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4u7qCUmY7_z"
      },
      "outputs": [],
      "source": [
        "# All ReLU layers' gradient distribution through after 100000 loops\n",
        "plt.figure(figsize=(20, 4))\n",
        "legends = []\n",
        "for i, layer in enumerate(layers[:-1]):\n",
        "  if isinstance(layer, relu):\n",
        "    t = layer.out.grad\n",
        "    print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean()*100))\n",
        "    hy, hx = torch.histogram(t, density=True)\n",
        "    plt.plot(hx[:-1].detach(), hy.detach())\n",
        "    legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
        "  plt.legend(legends);\n",
        "  plt.title(f'{layer.__class__.__name__} distribution')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oY8XkIVvY34o"
      },
      "outputs": [],
      "source": [
        "# All ReLU layers' output distribution through after 100000 loops - so bad T_T\n",
        "plt.figure(figsize=(20, 4))\n",
        "legends = []\n",
        "for i, layer in enumerate(layers[:-1]):\n",
        "  if isinstance(layer, relu):\n",
        "    t = layer.out\n",
        "    print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean()*100))\n",
        "    hy, hx = torch.histogram(t, density=True)\n",
        "    plt.plot(hx[:-1].detach(), hy.detach())\n",
        "    legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
        "plt.legend(legends);\n",
        "plt.title('Linear distribution')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ek1_NGlBY6pe"
      },
      "outputs": [],
      "source": [
        "# All Batchnorm layers' output distribution through after 100000 loops\n",
        "plt.figure(figsize=(20, 4))\n",
        "legends = []\n",
        "for i, layer in enumerate(layers[:-1]):\n",
        "  if isinstance(layer, BatchNorm1d):\n",
        "    t = layer.out\n",
        "    print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean()*100))\n",
        "    hy, hx = torch.histogram(t, density=True)\n",
        "    plt.plot(hx[:-1].detach(), hy.detach())\n",
        "    legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
        "plt.legend(legends);\n",
        "plt.title('BatchNorm distribution')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pl4IqAZDEQPT"
      },
      "outputs": [],
      "source": [
        "# Try making predictions on the training set with trained model\n",
        "for layer in layers:\n",
        "  layer.training = False\n",
        "\n",
        "out = xtr\n",
        "for layer in layers:\n",
        "  out = layer(out)\n",
        "\n",
        "y_pred = TargetVarScalerFit.inverse_transform(out.detach())\n",
        "ytr_orig = TargetVarScalerFit.inverse_transform(ytr)\n",
        "\n",
        "TestingData = pd.DataFrame(data=xtr, columns=x.columns)\n",
        "TestingData['Truth label'] = ytr_orig\n",
        "TestingData['Predicted label'] = y_pred\n",
        "TestingData[['Truth label', 'Predicted label']][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvS0aOheEjvo"
      },
      "outputs": [],
      "source": [
        "# Try making predictions on the testing set with trained model - maybe it's a bit overfitting T_T\n",
        "for layer in layers:\n",
        "  layer.training = False\n",
        "\n",
        "out = xte\n",
        "for layer in layers:\n",
        "  out = layer(out)\n",
        "\n",
        "y_pred = TargetVarScalerFit.inverse_transform(out.detach())\n",
        "yte_orig = TargetVarScalerFit.inverse_transform(yte)\n",
        "\n",
        "TestingData = pd.DataFrame(data=xte, columns=x.columns)\n",
        "TestingData['Truth label'] = yte_orig\n",
        "TestingData['Predicted label'] = y_pred\n",
        "TestingData[['Truth label', 'Predicted label']][:10]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UL3Fq2Prb0Vh",
        "293v4IUbcFxY",
        "O9wSA54EcW0T",
        "Ql0St7PhgiGK",
        "CdoONjzKcmx2",
        "QcOM625Beziq"
      ],
      "authorship_tag": "ABX9TyNRSMWVv/C+5aIyEbKLFreT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}